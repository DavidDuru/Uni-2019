---
title: "LabReport4 Cross validation and prediction"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tibble)
library(tidyverse)
#source("https://bioconductor.org/biocLite.R")
#biocLite("GEOquery")
library(Biobase)
library(GEOquery)
library(dplyr)
library(edgeR)
library(limma)
library(ggfortify)
#BiocManager::install("DESeq2")
library(DESeq2)
library(gplots)
library(RColorBrewer)
library(ggpubr)
require(caret)
require(e1071)
```

```{r load_data,results="hide",message=FALSE,warning=FALSE}
gds3057 <- getGEO(filename='GDS3057_full.soft.gz')
```

###Wrangling Data

I could not get the DEA from week 10 to work on my machine so, I used my DEA from the assignment.

It is gene expression data for healthy/unhealthy patients with leukemia.

```{r clean_data,results="hide",message=FALSE,warning=FALSE}
#these are some genes that were particularly highly expressed
#we'll try to find them using DEA
#up_regulated <- c("BIK", "CCNA1", "FUT4", "IL3RA", "HOMER3", "JAG1", "WT1")
#low expressed results from study
#down_regulated <- c("ALDHA1A", "PELO", "PLXNC1", "PRUNE", "SERPINB9", "TRIB2")
#Load in the soft file

#naming some stuff to use later
disease <- gds3057@dataTable@columns

#head(disease)

df <- gds3057@dataTable@table
eset <- df

#Subsetting just the important expression data
eset <- eset[,3:66]

#There is a key/value relationship between the genes
#Donm't know why, it results it duplicate genes being recorded
rname <- df$ID_REF

#renamingrows of expression set
rownames(eset) <- rname

#dropping unncessary info
#renaming rows of disease state
dRowNames <- disease$sample
disease <- disease %>% select(-sample)
disease <- disease %>% select(-cell.type,-description)
rownames(disease) <- dRowNames

#disease looks like this now
#head(disease)

#construct design matrix
#One hot encoding
design <- model.matrix(~disease.state, disease)

#looks like this
#head(design)
#head(eset)
```

```{r}
#Now we can do the same thing with DGE, see if we get better results
dge <- DGEList(counts = eset[, rownames(design)])
dge <- calcNormFactors(dge)

#Some form of variance stabilisation
v <- voom(dge, design, plot = TRUE)

#Fitting the linear model with the voom data
fit <- lmFit(v, design)
fit <- eBayes(fit, robust = TRUE)

#See how results differ from previous testing method
results <- decideTests(fit[, "disease.statenormal"])
#summary(results)

#I'm a bit worried about this histogram
#I think it should be uniform, but there's loads of 0's
hist(fit$p.value[, 2])

#where our p values sit around 0
volcanoplot(fit, coef = "disease.statenormal", highlight = 12, names = df$IDENTIFIER)

#A list of the top genes
topGenes = topTable(fit, coef = "disease.statenormal", n = 1e+06, p.value = 0.05)

########
#PROPER RESULTS HERE

#here are the pvalues
#topGenes
check_final <- rownames(topGenes)

#These are the features with the lowest p values
top_10_lowest_pvalues_ids <- check_final[1:10]

top_10_lowest_pvalues_ids

pvals <- topGenes$P.Value

#topGenes
#we have to look up which features correspond to which specific genes
Id_to_gene <- df %>% filter(df$ID_REF %in% top_10_lowest_pvalues_ids)

#you can see which id corresponds to which gene here
#Id_to_gene

genes <- Id_to_gene$IDENTIFIER
#genes
#up_regulated

#down_regulated
```
We transform the data in order to include the disease state variable which is a binary factor indicating if the patient has the disease or not.

I also used caret instead of the solution given, The confusionMatrix() function gives you information on your predictions.

The data is split into test and training sets, 30% and 70% respectively.

We perform 10 fold cross validation on a logsitic regression model that uses the gene that was most significantly expressed in our DEA as the parameter.

We get 86% predictive accuracy just checking the expression level of this single gene.

```{r}

#Id_to_gene

#disease$disease.state

test <- Id_to_gene[,-1]
test <- test[,1:65]
rownames(test) <- test$IDENTIFIER
test <- test %>% select(-IDENTIFIER)
test2 <- test
test <- t(test)
test <- as.data.frame(test)
#head(test)

#dim(test)

test <- cbind(disease.state = disease$disease.state, test)

df4 <- test

set.seed(3456)
trainIndex <- createDataPartition(df4$disease.state, p = .7, 
                                  list = FALSE, 
                                  times = 1)
dtrain<-df4[trainIndex,]
dtest<-df4[-trainIndex,]

fitControl <- trainControl(## 10-fold CV
  method = "cv",
  number = 10,
  savePredictions = TRUE
)

#dtrain
#dtest

## Logistic regression
lreg<-train(disease.state ~ ALDH1A1 , data=dtrain,method="glm",family=binomial(),
             trControl=fitControl)


lreg_pred<-predict(lreg,test)

confusionMatrix(lreg_pred,test$disease.state)

```
We can plot our predictions agaisnt our actual data to see where the model went wrong.
```{r}


p1  <- ggplot(test, aes(x = rownames(test), y = ALDH1A1, colour = disease.state)) +
  geom_point() + ggtitle("Actual results from data") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6))

p2 <- ggplot(test, aes(x = rownames(test), y = ALDH1A1, colour = lreg_pred)) +
  geom_point() + ggtitle("Prediction results using logsitic regression model, disease.state ~ ALDH1A1") +
 theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6))

ggarrange(p1,p2, nrow=2)


```





